See */test.ipynb for examples of evaluating our heuristics.

ACO implementations are adapted from [*DeepACO: Neural-enhanced Ant Systems for Combinatorial Optimization*](https://github.com/henry-yeh/DeepACO)

For POMO settings, download the checkpoints from its [official repository](https://github.com/yd-kwon/POMO) and place them in the corresponding directories. For example, place `checkpoint-3100.pt` for TSP at `problems/tsp_pomo/checkpoints/checkpoint-3100.pt`.

For LEHD settings, download the checkpoints and data from its [official repository](https://github.com/CIAM-Group/NCO_code/tree/main/single_objective/LEHD) and place them in the corresponding directories.

---

**Reference:**

[1] Ye, H., Wang, J., Cao, Z., Liang, H., & Li, Y. (2023). DeepACO: Neural-enhanced Ant Systems for Combinatorial Optimization. In Advances in Neural Information Processing Systems.

[2] Kwon, Y. D., Choo, J., Kim, B., Yoon, I., Gwon, Y., & Min, S. (2020). Pomo: Policy optimization with multiple optima for reinforcement learning. In Advances in Neural Information Processing Systems.